---
title: "Devoir_3"
author: "EL Hadrami N'DOYE"
date: "23/12/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library("FactoMineR")
library("factoextra")
library("corrplot")
```


$\underline{\text{Exercice 23}}$

$\textbf{1.ACP sur la main}$

```{r}
Z1 <- c(1:3,4,9)
Z2 <- c(5,10,rep(8,2),12)
n <- length(Z2)
mat <- matrix(c(Z1,Z2),nrow=2,ncol=5,byrow = TRUE,dimnames = list(c("Z1","Z2")))
meanZ1 <- mean(mat[1,])
meanZ2 <- mean(mat[2,])
meanZ1
meanZ2
miZ1 <- sd(mat[1,])
miZ2 <- sd(mat[2,])
Z1norm <- (Z1 - mean(Z1)) / sd(Z1)
Z2norm <- (Z2 - mean(Z2)) / sd(Z2)
matnorm <- matrix(c(Z1norm,Z2norm),nrow=2,ncol=5,byrow = TRUE,dimnames = list(c("Z1","Z2")))
# Matrice de correlation
matcorr <- (1/4) * (matnorm %*% t(matnorm))
matcorr
# valeurs propres et vecteurs propres
eig <- eigen(matcorr)
valp1 <- eig$values[1]
valp1
vp1 <- eig$vectors[,1]
vp1
valp2 <- eig$values[2]
valp2
vp2 <- eig$vectors[,2]
vp2
# Cercle de correlation contenat les vecteurs X1 et X2
X1 <-  sqrt(valp1) * vp1
X1
X2 <- sqrt(valp2) * vp2
X2
```
$\textbf{2.Interpretation}$

On retrouve les memes resultats trouvés dans le cours

$\textbf{3.Utilisation des commandes}$

```{r}
# Standardisation des données
s1 <- scale(x = Z1,center=TRUE,scale=TRUE)
s2 <- scale(x = Z2,center=TRUE,scale=TRUE)
mats <- matrix(c(s1,s2),nrow = 2,ncol=5,byrow = TRUE)
```

$\text{fonction gsvd}$

```{r}
gsvd <- function(Z,r,c){
  #Z matrice numerique de dimension (n,p) et de rang k
  #r poids de la metrique des lignes N=diag(r)
  # c poids de la metrique des colonnes M=diag(c)
  #-----sortie---------------
  # d vecteur de taille k contenant les valeurs singulieres (racines carres des valeurs propres)
  # U matrice de dimension (n,k) des vecteurs propres de de ZMZ'N
  # V matrice de dimension (p,k) des vecteurs propres de de Z'NZM
  k <-qr(Z)$rank
  colnames<-colnames(Z)
  rownames<-rownames(Z)
  Z <-as.matrix(Z)
  Ztilde <-diag(sqrt(r)) %*% Z %*%diag(sqrt(c))
  e <-svd(Ztilde)
  U <-diag(1/sqrt(r))%*%e$u[,1:k]# Attention : ne s'ecrit comme cela que parceque N et M sont diagonales!
  V <-diag(1/sqrt(c))%*%e$v[,1:k]
  d <- e$d[1:k]                                              
  rownames(U) <- rownames
  rownames(V) <- colnames
  if(length(d)>1)
    colnames(U) <-colnames(V) <-paste("dim", 1:k, sep = "")
  return(list(U=U,V=V,d=d))
}
r <-rep(1/nrow(mats),nrow(mats)) #lignes ponderees par 1/n
c <-rep(1,ncol(mats)) #colonnes ponderees par 1
U<- gsvd(mats,r,c)$U
V <- gsvd(mats,r,c)$V
d <-gsvd(mats,r,c)$d
U %*% diag(d) # Coordonnées de X
prcomp(mats)
PCA(mat,scale.unit = TRUE,graph = FALSE)$ind$coord
```

$\underline{\text{Exercice 24}}$

$\textbf{Chargement du jeux de données}$

```{r}
# load data
data_ski <- read.table("data/stations.txt",header = TRUE)
# extraction des variables quantitatives 
data_ski_active <- as.matrix(data_ski[1:32,2:7])
rownames(data_ski_active) <- data_ski$Nom
summary(data_ski_active)
```

$\textbf{Realisation d'une ACP}$

```{r}
pcaski <- PCA(data_ski_active,scale.unit = T,graph = FALSE)
# Visualisation des valeurs propres
valp <- pcaski$eig
```

$\textbf{Graphe des valeurs propres}$

```{r}
fviz_eig(pcaski, addlabels = TRUE, ylim = c(0, 50))
```

Les deux premières composantes principales expliquent 74% de la variation,donc les deux premiers axes peuvent etre acceptés.

$\textbf{Graphique des variables}$

```{r}
var <- get_pca_var(pcaski)
```

$\textbf{Coordonnées des variables}$

```{r}
var$coord
fviz_pca_var(pcaski,axes = c(1,2))
```

$\underline{\text{Interpretation}}$

- Les variables positivement corrélées sont regroupées
- Les variables négativement corrélées sont positionnées sur les côtés opposés de l’origine du graphique
(quadrants opposés).
- La distance entre les variables et l’origine mesure la qualité de représentation des variables, les variables
qui sont loin de l’origine sont bien représentées par l’ACP.

$\textbf{Qualité de representation des variables}$

```{r}
corrplot(var$cos2,is.corr = FALSE)
fviz_cos2(pcaski, choice = "var", axes = 1 :2)
```


```{r}
fviz_pca_var(pcaski, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE)
```

$\underline{\text{Interpretations}}$

Un cos2 élevé indique une bonne représentation de la variable sur les axes principaux en considération(comme
on peut le voir dans le graphe ci dessus),
dans ce cas la variable est positionnée à proximité de la circonférence
du cercle de corrélation.

```{r}
fviz_contrib(pcaski, choice = "var", axes = 1 :2)
```

$\textbf{Diagramme circulaire des variables contributives}$

```{r}
fviz_pca_var(pcaski, col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),alha.var="contrib" )
```

$\textbf{Interpretation}$

La ligne en pointillé rouge, sur le diagramme en barre des variable, indique la contribution moyenne attendue.
Donc les variables les plus contributives sont $\textbf{piste},\textbf{remontee} \ \text{et} \ \textbf{prixfort}$

$\textbf{Graphiques des individus}$

```{r}
ind <- get_pca_ind(pcaski)
fviz_pca_ind (pcaski, col.ind = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE) 
fviz_contrib(pcaski, choice = "ind", axes = 1 :2)
```
$\underline{\text{observations}}$

- Graphe1 : On constate que des groupes de ressemblance se forment au niveau des individus, par exemple les stations de ski ValCenis et LaNorma sont proches.

- Graphe2 : On constate que les skieurs choisisent plus les station de Ski (La Plagne,Tignes,Les Arcs,Flumet,Courchevel,Bonneval,LesAillons,Val Thorens,LesSaisies,Areches) que tous les autres station de ski.



$\textbf{Construction d'un biplot}$
```{r}
fviz_pca_biplot(pcaski,
repel = TRUE,col.var = "#2E9FDF", # Couleur des variables
col.ind = "#696969") # Couleur des individues )
```







