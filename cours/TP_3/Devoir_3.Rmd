---
title: "Devoir3"
author: "EL_Hadrami"
date: "23/12/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library("FactoMineR")
library("factoextra")
library("corrplot")
```


$\underline{\text{Exercice 1}}$

```{r}
Z1 <- c(1:3,4,9)
Z2 <- c(5,10,rep(8,2),12)
n <- length(Z2)
mat <- matrix(c(Z1,Z2),nrow=2,ncol=5,byrow = TRUE,dimnames = list(c("Z1","Z2")))
meanZ1 <- mean(mat[1,])
meanZ2 <- mean(mat[2,])
miZ1 <- sd(mat[1,])
miZ2 <- sd(mat[2,])
Z1norm <- (Z1 - mean(Z1)) / sd(Z1)
Z2norm <- (Z2 - mean(Z2)) / sd(Z2)
matnorm <- matrix(c(Z1norm,Z2norm),nrow=2,ncol=5,byrow = TRUE,dimnames = list(c("Z1","Z2")))
# Matrice de correlation
matcorr <- (1/4) * (matnorm %*% t(matnorm))
# valeurs propres et vecteurs propres
eig <- eigen(matcorr)
valp1 <- eig$values[1]
vp1 <- eig$vectors[,1]
valp2 <- eig$values[2]
vp2 <- eig$vectors[,2]
# Cercle de correlation contenat les vecteurs X1 et X2
X1 <-  sqrt(valp1) * vp1
X2 <- sqrt(valp2) * vp2
```

$\text{1.ACP sur la main}$

```{r}
df <- as.data.frame(mat)
res.pca <- PCA(df,scale.unit = TRUE ,graph = FALSE)
```

$\text{2. Interpretation}$

$\text{3.Utilisation des commandes}$

```{r}
# Standardisation des données
s1 <- scale(x = Z1,center=TRUE,scale=TRUE)
s2 <- scale(x = Z2,center=TRUE,scale=TRUE)
mats <- matrix(c(s1,s2),nrow = 2,ncol=5,byrow = TRUE)
```

$\text{fonction gsvd}$

```{r}
gsvd <- function(Z,r,c){
  #Z matrice numerique de dimension (n,p) et de rang k
  #r poids de la metrique des lignes N=diag(r)
  # c poids de la metrique des colonnes M=diag(c)
  #-----sortie---------------
  # d vecteur de taille k contenant les valeurs singulieres (racines carres des valeurs propres)
  # U matrice de dimension (n,k) des vecteurs propres de de ZMZ'N
  # V matrice de dimension (p,k) des vecteurs propres de de Z'NZM
  k <-qr(Z)$rank
  colnames<-colnames(Z)
  rownames<-rownames(Z)
  Z <-as.matrix(Z)
  Ztilde <-diag(sqrt(r)) %*% Z %*%diag(sqrt(c))
  e <-svd(Ztilde)
  U <-diag(1/sqrt(r))%*%e$u[,1:k]# Attention : ne s'ecrit comme cela que parceque N et M sont diagonales!
  V <-diag(1/sqrt(c))%*%e$v[,1:k]
  d <- e$d[1:k]                                              
  rownames(U) <- rownames
  rownames(V) <- colnames
  if(length(d)>1)
    colnames(U) <-colnames(V) <-paste("dim", 1:k, sep = "")
  return(list(U=U,V=V,d=d))
}
r <-rep(1/nrow(mats),nrow(mats)) #lignes ponderees par 1/n
c <-rep(1,ncol(mats)) #colonnes ponderees par 1
U <-gsvd(mats,r,c)$U
d <-gsvd(mats,r,c)$d
Psi <- U %*%diag(d)
#princomp(mat,cor=TRUE)
```

$\underline{\text{Exercice 2}}$

```{r}
# load data
data_ski <- read.table("data/stations.txt",header = TRUE)
# extraction des variables quantitatives 
data_ski_active <- as.matrix(data_ski[1:32,2:7])
rownames(data_ski_active) <- data_ski$Nom
summary(data_ski_active)
```

$\text{PCA}$

```{r}
pcaski <- PCA(data_ski_active,scale.unit = T,graph = FALSE)
# Visualisation des valeurs propres
valp <- pcaski$eig
```

$\textbf{Graphe des valeurs propres}$

```{r}
fviz_eig(pcaski, addlabels = TRUE, ylim = c(0, 50))
```

$\text{Les deux premières composantes principales expliquent}$ 74\% $\text{de la variation, donc les deux premiers axes peuvent etre acceptés}$

$\text{Graphique des variables}$

```{r}
var <- get_pca_var(pcaski)
```

$\textbf{Coordonnées des variables}$

```{r}
var$coord
fviz_pca_var(pcaski,axes = c(1,2))
```

$\textbf{Interpretation}$

- Les variables positivement corrélées sont regroupées
- Les variables négativement corrélées sont positionnées sur les côtés opposés de l’origine du graphique
(quadrants opposés).
- La distance entre les variables et l’origine mesure la qualité de représentation des variables, les variables
qui sont loin de l’origine sont bien représentées par l’ACP.

$$\textbf{Qualité de representation des variables}$$

```{r}
corrplot(var$cos2,is.corr = FALSE)
fviz_cos2(pcaski, choice = "var", axes = 1 :2)
```

$\textbf{Interpretations}$

$\text{Un cos2 élevé indique une bonne représentation de la variable sur les axes principaux en considération(comme
on peut le voir dans le graphe ci dessus), dans ce cas la variable est positionnée à proximité de la circonférence
du cercle de corrélation.}$

```{r}
fviz_pca_var(pcaski, col.var = "cos2",gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE)
```

$\textbf{Contributions des variables}$

```{r}
var$contrib
corrplot(var$contrib,is.corr = FALSE)
fviz_contrib(pcaski, choice = "var", axes = 1 :2, top = 6)
```
$\textbf{Interpretation}$

$\text{La ligne en pointillé rouge, sur le graphique ci-dessus, indique la contribution
moyenne attendue.} \\$
$\text{Donc les variables les plus contributives sont}$ $\textbf{piste}$,$\textbf{remontee}$ et $\textbf{prixfort}$

$\text{Diagramme circulaire des variables contributives}$

```{r}
fviz_pca_var(pcaski, col.var = "contrib",gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),alha.var="contrib" )
```

$\textbf{Graphiques des individus}$

```{r}
ind <- get_pca_ind(pcaski)
fviz_pca_ind (pcaski, col.ind = "cos2",gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE) 
fviz_contrib(pcaski, choice = "ind", axes = 1 :2)
```


$$\textbf{Biplot}$$
```{r}
fviz_pca_biplot(pcaski,
repel = TRUE,
col.var = "#2E9FDF", # Couleur des variables
col.ind = "#696969") # Couleur des individues )
```

